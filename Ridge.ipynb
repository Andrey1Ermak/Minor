{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\beta) = \\|y - X\\beta\\|^2 + \\lambda \\|\\beta\\|^2$\n",
    "\n",
    "\n",
    "$\\beta = (X^T X + \\alpha I)^{-1} X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope: [2.71023266]\n",
      "intercept: 109.34970109497377\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([25, 45, 55, 70, 90, 100, 120, 150]).reshape(-1, 1)  \n",
    "y = np.array([150, 250, 270, 300, 350, 380, 450, 500]) \n",
    "\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "alpha = 0.9\n",
    "\n",
    "n, m = X_b.shape\n",
    "I = np.eye(m)\n",
    "I[0, 0] = 0  # Не регуляризуем intercept\n",
    "\n",
    "beta = np.linalg.inv(X_b.T.dot(X_b) + alpha * I).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"slope:\", beta[1:])\n",
    "print(\"intercept:\", beta[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- λ=0: штраф отключается, и Ridge-регрессия сводится к обычной линейной регрессии, может привести к переобучению. \n",
    "- λ становится больше: регуляризация начинает влиять сильнее, уменьшая значения коэффициентов. Модель становится более устойчивой к шуму в данных, но может быть недообучение. \n",
    "- λ→∞: все коэффициенты стремятся к 0. Модель становится крайне простой, пренебрегая информацией в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Контроль коррелированных признаков с помощью L2: Ridge предотвращает ситуацию, при которой один из сильно коррелированных признаков может занять доминирующую роль в модели, а другой будет проигнорирован. Вместо этого оба признака получают более сбалансированные и уменьшенные веса. Например, если два признака сильно коррелируют, модель не будет отдавать предпочтение только одному из них, как это происходит в случае с Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналитическое решение для Lasso-регрессии (L1-регуляризации) не существует, поскольку функция потерь включает абсолютное значение коэффициентов (модуль вместо квадрата), что делает оптимизационную задачу недифференцируемой в точках, где коэффициенты равны 0.\n",
    "\n",
    "Зато Lasso может занулять коэффициенты (то есть они не стремтся к 0, а фактически могут стать нулями у некоторых признаков) -> можно использовать для feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
